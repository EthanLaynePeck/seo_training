{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78a2220-23c6-4bc2-b043-a506df8df8bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Session 2: Functions and Basic Web Scraping in Python for SEO\n",
    "\n",
    "## Agenda\n",
    "- Functions\n",
    "    - What is a Function?\n",
    "    - How to Define and Call a Function\n",
    "    - Return Statements\n",
    "    - Components of a Function\n",
    "    - Example: Function to Calculate Square of a Number\n",
    "    - Understanding Docstrings\n",
    "        - What is a Docstring?\n",
    "- Setting Up a Virtual Environment for SEO Web Scraping\n",
    "    - Why Use Virtual Environments?\n",
    "    - Steps to Create the Conda Environment and Add to Jupyter Kernel List\n",
    "- Basics of Web Scraping\n",
    "    - What is Web Scraping?\n",
    "    - Introduction to `requests`\n",
    "    - Introduction to `BeautifulSoup`\n",
    "    - Introduction to `newspaper3k`\n",
    "- Build a Simple Web Scraper\n",
    "    - Walkthrough: Fetching and Parsing a Web Page\n",
    "    - Extracting SEO-relevant Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec453a-80e7-4ca8-9e47-5c4771636e23",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "### What is a Function?\n",
    "A function in Python is a reusable piece of code that performs a specific task. Functions are essential for code reusability and organization.\n",
    "\n",
    "### How to Define and Call a Function\n",
    "In Python, functions are defined using the `def` keyword followed by the function name and parentheses `()`. The code block within a function is executed when the function is called.\n",
    "\n",
    "### Return Statements\n",
    "The `return` keyword is used to exit a function and return a value.\n",
    "\n",
    "### Components of a Function\n",
    "- `def` keyword: To start the function definition\n",
    "- Function name: To identify the function\n",
    "- Parameters: Variables to pass into the function (optional)\n",
    "- Code block: The actions to perform\n",
    "- `return` keyword: To return a value (optional)\n",
    "\n",
    "#### Example: Function to Calculate Square of a Number\n",
    "```python\n",
    "# Definition\n",
    "def square_number(num):\n",
    "    \"\"\"This function returns the square of a number.\"\"\"\n",
    "    result = num * num\n",
    "    return result\n",
    "\n",
    "# Calling the Function\n",
    "square_of_five = square_number(5)\n",
    "print(f\"The square of 5 is {square_of_five}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c51085c-1e79-4983-b1f8-23044b5d26c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T15:53:37.193605Z",
     "iopub.status.busy": "2023-10-05T15:53:37.193332Z",
     "iopub.status.idle": "2023-10-05T15:53:37.198636Z",
     "shell.execute_reply": "2023-10-05T15:53:37.198249Z",
     "shell.execute_reply.started": "2023-10-05T15:53:37.193587Z"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "### Exercises\n",
    "\n",
    "#### 1. Create a function that takes a number and returns its cube.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cba16-6adf-4354-a66a-25a1be6aa499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde7369-1ef0-4b34-bca5-1adf092288d6",
   "metadata": {},
   "source": [
    "#### 2. Create a function that takes two numbers and returns their sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195bfc0-8a99-4f31-8dc7-70e3a1a8df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7da41-c8cb-4be5-b913-d35eca6f07b7",
   "metadata": {},
   "source": [
    "#### 3. Create a function that reverses a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ffe8f-fed9-44ec-a0a9-0ae6ea4ac3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e2665f-f150-428b-a2fb-1ac10881d479",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Understanding Docstrings\n",
    "\n",
    "#### What is a Docstring?\n",
    "A docstring is a string literal that occurs as the first statement in a module, function, class, or method definition. In Python, docstrings are used for documentation and can be accessed at runtime using the `help()` function. You can also view the docstring by placing your cursor inside the function's parentheses and pressing Shift + Tab in a Jupyter Notebook.\n",
    "\n",
    "#### How to Write a Docstring\n",
    "To write a docstring, you enclose your descriptive text in triple quotes, either single (`''' ... '''`) or double (`\"\"\" ... \"\"\"`). It should be placed right after the function definition. \n",
    "\n",
    "#### Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4530b-5fec-42d4-9b87-f9c75567981a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "def add_numbers(a, b):\n",
    "    \"\"\"\n",
    "    This function takes two numbers and returns their sum.\n",
    "    Parameters:\n",
    "    - a: first number\n",
    "    - b: second number\n",
    "    \n",
    "    Returns:\n",
    "    Sum of a and b\n",
    "    \"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d5abf-9bbe-41ea-adbe-67d683d3deef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click Shift + Tab inside the parentheses\n",
    "\n",
    "add_numbers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4b5e1e-8d2c-4c8a-95f7-62ef2b6b7c51",
   "metadata": {},
   "source": [
    "---\n",
    "## Setting Up a Virtual Environment for SEO Web Scraping\n",
    "\n",
    "### Why Use Virtual Environments?\n",
    "\n",
    "Virtual environments are isolated spaces where you can install software and Python packages independently of the system-wide Python installation. Using a virtual environment is advantageous for several reasons:\n",
    "\n",
    "1. **Isolation**: You can isolate your project's dependencies to avoid version conflicts.\n",
    "2. **Reproducibility**: Makes it easier to share your code and environment setup with others.\n",
    "3. **Simplicity**: Simplifies your system Python setup by allowing you to only install packages needed for each specific project.\n",
    "\n",
    "### Steps to Create the Conda Environment and Add to Jupyter Kernel List\n",
    "\n",
    "#### Create the Conda Environment\n",
    "\n",
    "First, save the following YAML configuration into a file named `environment.yml`.\n",
    "\n",
    "```yaml\n",
    "name: seo_webscraper\n",
    "channels:\n",
    "  - defaults\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - requests\n",
    "  - beautifulsoup4\n",
    "  - pandas\n",
    "  - selenium\n",
    "  - lxml\n",
    "  - scikit-learn\n",
    "  - matplotlib\n",
    "  - jupyter\n",
    "  - ipykernel\n",
    "  - newspaper3k\n",
    "```\n",
    "\n",
    "After saving the `environment.yml`, open your terminal and navigate to the folder containing the file. Then run:\n",
    "\n",
    "```bash\n",
    "conda env create\n",
    "```\n",
    "\n",
    "#### Activate the Conda Environment\n",
    "\n",
    "To activate the environment, run:\n",
    "\n",
    "```bash\n",
    "conda activate seo_webscraper\n",
    "```\n",
    "\n",
    "#### Add the Environment to Jupyter Kernel List\n",
    "\n",
    "To make this environment accessible in Jupyter Notebook or Jupyter Lab, add it to your kernel list:\n",
    "\n",
    "```bash\n",
    "python -m ipykernel install --user --name=seo_webscraper --display-name=\"SEO Webscraper\"\n",
    "```\n",
    "\n",
    "Now, you should be able to switch to this kernel while working in Jupyter and have access to all the packages you've specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07291821-f5bb-4967-aa43-857caf6fc8c9",
   "metadata": {},
   "source": [
    "<video controls src=\"media/verify_kernel.mp4\" width=\"800\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cecf15b-c1c4-4735-a5e4-911b506936d5",
   "metadata": {},
   "source": [
    "---\n",
    "## Basics of Web Scraping\n",
    "\n",
    "Web scraping is an essential skill for SEO and data analysis. It involves fetching a web page and then extracting necessary information. In Python, libraries like `requests` and `BeautifulSoup` are commonly used for web scraping.\n",
    "\n",
    "### What is Web Scraping?\n",
    "\n",
    "Web scraping is the process of extracting data from websites. It's essentially a way to collect information from the web programmatically, as opposed to manually browsing and collecting data.\n",
    "\n",
    "### Introduction to `requests`\n",
    "\n",
    "The `requests` library is one of the most popular Python libraries for making HTTP requests to a specified URL. With `requests`, you can send HTTP/1.1 requests and handle the response to get the web content.\n",
    "\n",
    "---\n",
    "### Example of using `requests`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c6845-e974-4819-8373-3fb5a06908e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('http://www.example.com')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae32d96-8de3-405a-8ab1-95d68a2caa85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Introduction to `BeautifulSoup`\n",
    "\n",
    "`BeautifulSoup` is a Python library used for web scraping purposes to pull data out of HTML and XML files. It creates a parse tree from the page source code that can be used to extract data in a hierarchical and more readable manner.\n",
    "\n",
    "---\n",
    "#### Example of using BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad265d2b-822f-400d-a6e4-85e88a0c7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "response = requests.get('http://www.example.com')\n",
    "\n",
    "# Initialize BeautifulSoup object with 'lxml' parser for efficient HTML parsing\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# Header and separator for the first <h1> tag\n",
    "print(\"------\")\n",
    "print(\"First <h1> Tag:\")\n",
    "print(\"------\")\n",
    "\n",
    "# Find the first <h1> tag on the page\n",
    "first_h1 = soup.find('h1')\n",
    "print(first_h1.text)\n",
    "\n",
    "# Add some space for readability\n",
    "print(\"\\n\")\n",
    "\n",
    "# Header and separator for the first <p> tag\n",
    "print(\"------\")\n",
    "print(\"First <p> Tag:\")\n",
    "print(\"------\")\n",
    "\n",
    "# Find the first <p> tag on the page\n",
    "first_p = soup.find('p')\n",
    "print(first_p.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7bdeb-a12e-4c97-bd66-7ea906be9f2c",
   "metadata": {},
   "source": [
    "---\n",
    "In this example above, we used `BeautifulSoup` along with the `requests` library to fetch a web page. We then demonstrated two different operations:\n",
    "\n",
    "1. Finding the first `<h1>` tag on the page using the `find()` method. The method returns the first occurrence of the specified HTML tag.\n",
    "   \n",
    "2. Finding the first `<p>` (paragraph) tag on the page, again using the `find()` method.\n",
    "\n",
    "Both operations show the simplicity and power of using `BeautifulSoup` for web scraping tasks. With these basic techniques, you'll be well-prepared to start building your own web scrapers for gathering data for SEO or other analytical purposes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f83e381-7493-4cef-b644-2e16182ff215",
   "metadata": {},
   "source": [
    "### Introduction to `newspaper3k`\n",
    "\n",
    "`newspaper3k` is a Python library designed for web scraping articles from various news outlets or any textual content from web pages. It can handle article parsing, natural language processing, and even downloading images. The library is highly versatile and makes it easy to collect structured information from the web, making it a valuable tool for SEO analysis, data mining, or content aggregation.\n",
    "\n",
    "\n",
    "---\n",
    "#### Example of Using `newspaper3k`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23d0938-6542-4345-9ef1-096c6a4089d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "# Specify the URL of the article to scrape\n",
    "url = \"http://example.com/\"\n",
    "\n",
    "# Create an Article object\n",
    "article = Article(url)\n",
    "\n",
    "# Download and parse the article\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "# Print the article's text\n",
    "print(article.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7a5b4d-99d7-4b17-9189-b5c2d80d2989",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this example, we used the newspaper3k library to download, parse, and print the text of an article from a given URL. This shows how straightforward it is to gather text-based data for SEO or other analytical purposes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c472be52-0c59-460e-b212-495d0fd9a574",
   "metadata": {},
   "source": [
    "## Build a Simple Web Scraper\n",
    "\n",
    "In this section, we will put our newfound knowledge into practice. We will build a simple web scraper that will scrape multiple websites.\n",
    "\n",
    "### Objectives:\n",
    "\n",
    "- Create a Python function to handle the web scraping.\n",
    "- Use the function to scrape information from a list of websites.\n",
    "\n",
    "### Exercise Steps:\n",
    "\n",
    "1. **Find Three Websites**: List down URLs of three websites you're interested in scraping.\n",
    "  \n",
    "2. **Initialize URL List**: Put these URLs in a Python list.\n",
    "\n",
    "3. **Create a Web Scraping Function**: Write a function that will take a URL as an input and perform the following tasks:\n",
    "    - Fetch the web page using the `requests` library.\n",
    "    - Parse the page using `BeautifulSoup`.\n",
    "    - Extract and print the title of the web page.\n",
    "    - Extract and print the first paragraph (`<p>` tag) of the web page.\n",
    "    - Identify and print the number of outbound links in the page (count the number of `<a>` tags).\n",
    "    \n",
    "4. **Iterate Over URL List**: Use a loop to iterate over the list of URLs. For each URL, call the web scraping function you created.\n",
    "\n",
    "5. **Observe the Output**: Review the data that your function has gathered from each website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83948cd4-9b9b-4577-abb4-02b2038d7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the list of URLs\n",
    "urls = [\n",
    "    'http://example.com/first-website',\n",
    "    'http://example.com/second-website',\n",
    "    'http://example.com/third-website'\n",
    "]\n",
    "\n",
    "# Define the web scraping function\n",
    "def scrape_website(url):\n",
    "    # TODO: Fetch the web page using 'requests'\n",
    "    \n",
    "    # TODO: Parse the page using 'BeautifulSoup'\n",
    "    \n",
    "    # TODO: Extract and print the title of the web page\n",
    "    \n",
    "    # TODO: Extract and print the first paragraph (<p> tag) of the web page\n",
    "    \n",
    "    # TODO: Identify and print the number of outbound links (count the <a> tags)\n",
    "\n",
    "# Iterate over the list of URLs and call the web scraping function for each\n",
    "for url in urls:\n",
    "    print(f\"Scraping {url}...\")\n",
    "    scrape_website(url)\n",
    "    print(\"---------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1c4e1-41e0-40ee-b148-b1c950e37a01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Completed Example with the Newspaper package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61217d7-711c-45e4-b92d-cfc511a4e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the newspaper library\n",
    "from newspaper import Article\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    'https://veteran.com/va-loans-disability-rating/',\n",
    "    'https://veteran.com/housing-home-ownership/',\n",
    "    'https://veteran.com/va-owned-for-sale/'\n",
    "]\n",
    "\n",
    "# Define the web scraping function using newspaper3k\n",
    "def scrape_article(url):\n",
    "    # Create an Article object\n",
    "    article = Article(url)\n",
    "    \n",
    "    # Download and parse the article\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    \n",
    "    # Print the article's title\n",
    "    print(f\"Title: {article.title}\")\n",
    "    \n",
    "    # Print the number of images in the article\n",
    "    print(f\"Number of Images: {len(article.images)}\")\n",
    "\n",
    "# Loop through each URL in the list\n",
    "for url in urls:\n",
    "    print(f\"Scraping {url}...\")\n",
    "    scrape_article(url)\n",
    "    print(\"---------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEO Webscraper",
   "language": "python",
   "name": "seo_webscraper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
